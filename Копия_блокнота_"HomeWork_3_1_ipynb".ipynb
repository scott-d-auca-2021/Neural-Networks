{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scott-d-auca-2021/Neural-Networks/blob/main/%D0%9A%D0%BE%D0%BF%D0%B8%D1%8F_%D0%B1%D0%BB%D0%BE%D0%BA%D0%BD%D0%BE%D1%82%D0%B0_%22HomeWork_3_1_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0c7697f",
      "metadata": {
        "id": "b0c7697f"
      },
      "source": [
        "## Problems 1 (2 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6694bba1",
      "metadata": {
        "id": "6694bba1"
      },
      "source": [
        "In this homework we will implement a forward pass of the convolutional layer. We will not touch the reverse pass and calculation of derivatives.\n",
        "\n",
        "Let's remember how the convolutional layer works:\n",
        "\n",
        "* an array of images is supplied as input also called a batch\n",
        "* zeros are added to each image along the borders\n",
        "* each of the filters of the convolutional layer “slides” over each image\n",
        "\n",
        "***Let's start with a warm-up - we'll implement a function that adds padding.***\n",
        "\n",
        "Let us have an input_images batch of two images with three channels (RGB). Let the size of the images be 3*3. Recall that the input of the convolutional layer has the following dimension:\n",
        "\n",
        "* batch size\n",
        "* number of channels\n",
        "* height\n",
        "* width\n",
        "\n",
        "In the case under consideration, the input dimension is (2, 3, 3, 3).\n",
        "\n",
        "If we add a padding of one zero around each image then the size of each image will become 3 + 2 * 1 = 5 pixels wide and 5 pixels high respectively (add one zero on each side of the image).\n",
        "\n",
        "Write any working implementation.\n",
        "\n",
        "![](https://ucarecdn.com/b4f16f35-13a7-4740-9760-075a708382c3/-/crop/463x266/0,168/-/preview/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e75e7dc9",
      "metadata": {
        "id": "e75e7dc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b193a100-ba3e-4a06-a8b7-0a07ba153cd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Create an input array from two RGB 3*3 images\n",
        "input_images = torch.tensor(\n",
        "      [[[[0,  1,  2],\n",
        "         [3,  4,  5],\n",
        "         [6,  7,  8]],\n",
        "\n",
        "        [[9, 10, 11],\n",
        "         [12, 13, 14],\n",
        "         [15, 16, 17]],\n",
        "\n",
        "        [[18, 19, 20],\n",
        "         [21, 22, 23],\n",
        "         [24, 25, 26]]],\n",
        "\n",
        "\n",
        "       [[[27, 28, 29],\n",
        "         [30, 31, 32],\n",
        "         [33, 34, 35]],\n",
        "\n",
        "        [[36, 37, 38],\n",
        "         [39, 40, 41],\n",
        "         [42, 43, 44]],\n",
        "\n",
        "        [[45, 46, 47],\n",
        "         [48, 49, 50],\n",
        "         [51, 52, 53]]]])\n",
        "\n",
        "\n",
        "def get_padding2d(input_images):\n",
        "     batch_size, num_channels, height, width = input_images.shape\n",
        "     padding_size = 1\n",
        "\n",
        "     # Новый тензор с нулями, но без данных\n",
        "     padded_images = torch.zeros(\n",
        "         batch_size, num_channels, height + 2 * padding_size, width + 2 * padding_size\n",
        "     )\n",
        "\n",
        "     # Переносим данные в центр ноликов\n",
        "     padded_images[:, :, padding_size : height + padding_size, padding_size : width + padding_size] = input_images\n",
        "\n",
        "     return padded_images\n",
        "\n",
        "\n",
        "correct_padded_images = torch.tensor(\n",
        "       [[[[0.,  0.,  0.,  0.,  0.],\n",
        "          [0.,  0.,  1.,  2.,  0.],\n",
        "          [0.,  3.,  4.,  5.,  0.],\n",
        "          [0.,  6.,  7.,  8.,  0.],\n",
        "          [0.,  0.,  0.,  0.,  0.]],\n",
        "\n",
        "         [[0.,  0.,  0.,  0.,  0.],\n",
        "          [0.,  9., 10., 11.,  0.],\n",
        "          [0., 12., 13., 14.,  0.],\n",
        "          [0., 15., 16., 17.,  0.],\n",
        "          [0.,  0.,  0.,  0.,  0.]],\n",
        "\n",
        "         [[0.,  0.,  0.,  0.,  0.],\n",
        "          [0., 18., 19., 20.,  0.],\n",
        "          [0., 21., 22., 23.,  0.],\n",
        "          [0., 24., 25., 26.,  0.],\n",
        "          [0.,  0.,  0.,  0.,  0.]]],\n",
        "\n",
        "\n",
        "        [[[0.,  0.,  0.,  0.,  0.],\n",
        "          [0., 27., 28., 29.,  0.],\n",
        "          [0., 30., 31., 32.,  0.],\n",
        "          [0., 33., 34., 35.,  0.],\n",
        "          [0.,  0.,  0.,  0.,  0.]],\n",
        "\n",
        "         [[0.,  0.,  0.,  0.,  0.],\n",
        "          [0., 36., 37., 38.,  0.],\n",
        "          [0., 39., 40., 41.,  0.],\n",
        "          [0., 42., 43., 44.,  0.],\n",
        "          [0.,  0.,  0.,  0.,  0.]],\n",
        "\n",
        "         [[0.,  0.,  0.,  0.,  0.],\n",
        "          [0., 45., 46., 47.,  0.],\n",
        "          [0., 48., 49., 50.,  0.],\n",
        "          [0., 51., 52., 53.,  0.],\n",
        "          [0.,  0.,  0.,  0.,  0.]]]])\n",
        "\n",
        "\n",
        "print(torch.allclose(get_padding2d(input_images), correct_padded_images))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b361224d",
      "metadata": {
        "id": "b361224d"
      },
      "source": [
        "## Problems 2 (3 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df654b0a",
      "metadata": {
        "id": "df654b0a"
      },
      "source": [
        "In this task, we will consider what the convolutional layer consists of.\n",
        "\n",
        "A convolutional layer is an array of filters.\n",
        "\n",
        "Each filter has the following dimension:\n",
        "* number of layers in the input image (for RGB this is 3)\n",
        "* filter height\n",
        "* filter width\n",
        "\n",
        "In the kernel, all filters have the same dimensions so the width and height of the filters are called the kernel width and height. Most often, the width of the kernel is equal to the height of the kernel in which case they are called the kernel size (kernel_size).\n",
        "\n",
        "\n",
        "The layer also has the following parameters:\n",
        "\n",
        "* padding - how many pixels to increase the input image on each side.\n",
        "\n",
        "* stride - how many pixels the filter is shifted by when calculating the convolution\n",
        "\n",
        "\n",
        "\n",
        "Try to derive the formula for the output dimension of the convolutional layer yourself, knowing the input and kernel parameters.\n",
        "\n",
        "Check the correctness of the formula by comparing it with the formula from the [documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d).\n",
        "\n",
        "\n",
        "To make sure your formula is correct write a function that takes as input:\n",
        "* input dimension (number of images in a batch*number of layers in one image*image height*image width)\n",
        "* number of filters\n",
        "* filter size (we assume that the height is the same as the width)\n",
        "* padding\n",
        "* stride\n",
        "\n",
        "The function must return the output dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f6c8d3f",
      "metadata": {
        "id": "6f6c8d3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20bc06fa-a007-4bf6-b1b7-50a6b66204df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def calc_out_shape(input_matrix_shape, out_channels, kernel_size, stride, padding):\n",
        "    out_shape = [input_matrix_shape[0], out_channels, ((input_matrix_shape[2] - kernel_size + 2 * padding) / stride) + 1, ((input_matrix_shape[2] - kernel_size + 2 * padding) / stride) + 1]\n",
        "\n",
        "    return out_shape\n",
        "\n",
        "print(np.array_equal(\n",
        "    calc_out_shape(input_matrix_shape=[2, 3, 10, 10],\n",
        "                   out_channels=10,\n",
        "                   kernel_size=3,\n",
        "                   stride=1,\n",
        "                   padding=0),\n",
        "    [2, 10, 8, 8]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "505f2a22",
      "metadata": {
        "id": "505f2a22"
      },
      "source": [
        "## Problem 3 (5 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47638ca5",
      "metadata": {
        "id": "47638ca5"
      },
      "source": [
        "Let's reuse the code from the previous step to test our implementation of the convolutional layer.\n",
        "\n",
        "Let's consider convolving a batch of one single-layer 3*3 image with a kernel of one 2*2 filter, stride = 1, that is the output should be one 2*2 matrix. The strictly written output dimension is equal to (1 - images in the batch, 1 - number of filters in the kernel, 2 - height of the output matrix, 2 - width of the output matrix).\n",
        "\n",
        "Let W be the kernel weights, X the input, Y the output.\n",
        "![](https://ucarecdn.com/fe231533-bcb9-40c6-a589-46b515991c35/)\n",
        "\n",
        "You can calculate the output in a loop:\n",
        "![](https://ucarecdn.com/2eef930e-2afe-420a-8965-96acc95a6139/)\n",
        "\n",
        "At each iteration of the loop, the filter is multiplied pixel by pixel by part of the image, and then the 4 resulting numbers are added up to produce one pixel of output.\n",
        "\n",
        "The required number of iterations for this case is 4, since there can be 2 positions of the core and 2 vertically, the total number of iterations is the product of the numbers of positions, that is, in this case 2*2 = 4.\n",
        "\n",
        "Let's move from the simple case to the general one.\n",
        "\n",
        "* ***If the image were multi-layered*** for example three-layered - RGB, then the filters in the core should also be three-layered. Each filter layer is multiplied pixel-by-pixel by the corresponding layer of the original image. That is, in this case, after multiplication, the result would be 4 * 3 = 12 products, the results of which are added up, and the value of the output pixel is obtained.\n",
        "\n",
        "* ***If there were more than one filters in the kernel*** then an outer filter loop would be added, inside which we calculate the convolution for each filter.\n",
        "\n",
        "* ***If there were more than 1 image in the input batch*** then another outer loop would be added over the images in the batch.\n",
        "\n",
        "Reminder: In all steps of this tutorial, we consider the bias in the convolution layers to be zero.\n",
        "\n",
        "This problem requires implementing a convolutional layer through loops.\n",
        "\n",
        "Please note that the code considers the general case - the input batch does not necessarily consist of one image; there are several layers in the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d54ba0b7",
      "metadata": {
        "id": "d54ba0b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dd9ad34-9268-475e-d9be-53e301c68f68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "\n",
        "def calc_out_shape(input_matrix_shape, out_channels, kernel_size, stride, padding):\n",
        "    batch_size, channels_count, input_height, input_width = input_matrix_shape\n",
        "    output_height = (input_height + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
        "    output_width = (input_width + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
        "\n",
        "    return batch_size, out_channels, output_height, output_width\n",
        "\n",
        "# abstract class for convolution layer\n",
        "class ABCConv2d(ABC):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def set_kernel(self, kernel):\n",
        "        self.kernel = kernel\n",
        "\n",
        "    @abstractmethod\n",
        "    def __call__(self, input_tensor):\n",
        "        pass\n",
        "\n",
        "# wrapper class over torch.nn.Conv2d to unify the interface\n",
        "class Conv2d(ABCConv2d):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n",
        "                                      stride, padding=0, bias=False)\n",
        "\n",
        "    def set_kernel(self, kernel):\n",
        "        self.conv2d.weight.data = kernel\n",
        "\n",
        "    def __call__(self, input_tensor):\n",
        "        return self.conv2d(input_tensor)\n",
        "\n",
        "# function that creates an object of class cls and returns a convolution from input_matrix\n",
        "def create_and_call_conv2d_layer(conv2d_layer_class, stride, kernel, input_matrix):\n",
        "    out_channels = kernel.shape[0]\n",
        "    in_channels = kernel.shape[1]\n",
        "    kernel_size = kernel.shape[2]\n",
        "\n",
        "    layer = conv2d_layer_class(in_channels, out_channels, kernel_size, stride)\n",
        "    layer.set_kernel(kernel)\n",
        "\n",
        "    return layer(input_matrix)\n",
        "\n",
        "# Function that tests the conv2d_cls class.\n",
        "# Returns True if the convolution matches the convolution using torch.nn.Conv2d.\n",
        "def test_conv2d_layer(conv2d_layer_class, batch_size=2,\n",
        "                      input_height=4, input_width=4, stride=2):\n",
        "    kernel = torch.tensor(\n",
        "                      [[[[0., 1, 0],\n",
        "                         [1,  2, 1],\n",
        "                         [0,  1, 0]],\n",
        "\n",
        "                        [[1, 2, 1],\n",
        "                         [0, 3, 3],\n",
        "                         [0, 1, 10]],\n",
        "\n",
        "                        [[10, 11, 12],\n",
        "                         [13, 14, 15],\n",
        "                         [16, 17, 18]]]])\n",
        "\n",
        "    in_channels = kernel.shape[1]\n",
        "\n",
        "    input_tensor = torch.arange(0, batch_size * in_channels *\n",
        "                                input_height * input_width,\n",
        "                                out=torch.FloatTensor()) \\\n",
        "        .reshape(batch_size, in_channels, input_height, input_width)\n",
        "\n",
        "    custom_conv2d_out = create_and_call_conv2d_layer(\n",
        "        conv2d_layer_class, stride, kernel, input_tensor)\n",
        "    conv2d_out = create_and_call_conv2d_layer(\n",
        "        Conv2d, stride, kernel, input_tensor)\n",
        "\n",
        "    return torch.allclose(custom_conv2d_out, conv2d_out) \\\n",
        "             and (custom_conv2d_out.shape == conv2d_out.shape)\n",
        "\n",
        "\n",
        "# Convolutional layer through loops.\n",
        "class Conv2dLoop(ABCConv2d):\n",
        "    def __call__(self, input_tensor):\n",
        "        batch_size, in_channels, input_height, input_width = input_tensor.shape\n",
        "        # вызвали готовые методы выше\n",
        "        out_height = calc_out_shape(\n",
        "            input_matrix_shape=input_tensor.shape,\n",
        "            out_channels=self.out_channels,\n",
        "            kernel_size=self.kernel_size,\n",
        "            stride=self.stride,\n",
        "            padding=0)[2]\n",
        "        out_width = calc_out_shape(\n",
        "            input_matrix_shape=input_tensor.shape,\n",
        "            out_channels=self.out_channels,\n",
        "            kernel_size=self.kernel_size,\n",
        "            stride=self.stride,\n",
        "            padding=0)[3]\n",
        "\n",
        "        # заполнили нулями основу для аутпута\n",
        "        output_tensor = torch.zeros(\n",
        "            batch_size, self.out_channels, out_height, out_width)\n",
        "\n",
        "        for batch in range(batch_size):\n",
        "            for channel_out in range(self.out_channels):\n",
        "                for height_out in range(out_height):\n",
        "                    for weight_out in range(out_width):\n",
        "                        # отмерили где будет начинаться и кончаться h and w\n",
        "                        height_begin = height_out * self.stride\n",
        "                        height_end = height_begin + self.kernel_size\n",
        "\n",
        "                        weight_begin = weight_out * self.stride\n",
        "                        weight_end = weight_begin + self.kernel_size\n",
        "\n",
        "                        # отмеряем участок куда будет применять kernel\n",
        "                        patch = input_tensor[batch, :, height_begin:height_end, weight_begin:weight_end]\n",
        "                        # проводим умножение, складываем и вкладываем все в наш аут тензор\n",
        "                        output_tensor[batch, channel_out, height_out, weight_out] = torch.sum(\n",
        "                            patch * self.kernel[channel_out, :, :, :])\n",
        "\n",
        "        return output_tensor\n",
        "\n",
        "print(test_conv2d_layer(Conv2dLoop))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "361e986c",
      "metadata": {
        "id": "361e986c"
      },
      "source": [
        "## Problems 4 (5 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99833352",
      "metadata": {
        "id": "99833352"
      },
      "source": [
        "Let's reuse the code from the third task to test our implementation of the convolutional layer.\n",
        "\n",
        "Implementation through loops is very performance inefficient. There are actually two ways to do the same thing using matrix multiplication.\n",
        "\n",
        "This step will implement the first of them.\n",
        "\n",
        "Consider the convolution of one single-channel image of size 4*4 pixels (pixel values are denoted by X).\n",
        "\n",
        "We will collapse with a core of one filter of size 3*3, the weights are designated by W.\n",
        "\n",
        "For simplicity, let's assume stride = 1.\n",
        "\n",
        "Then the output Y will have a dimension of 1*1*2*2 (in this case, one image at the input is the first unit in the dimension, one filter in the kernel is the second unit in the output dimension).\n",
        "![](https://ucarecdn.com/1845714a-9187-4dca-83ef-6fe44f030391/-/crop/760x275/0,198/-/preview/)\n",
        "\n",
        "It turns out that the convolution output can be obtained by matrix multiplication as shown below.\n",
        "![](https://ucarecdn.com/e2a38490-d886-47d6-97b0-dc65f8906ba1/)\n",
        "\n",
        "***We recommend checking this by multiplying the matrices on the piece of paper.***\n",
        "\n",
        "Let's move from the simple case to the general one:\n",
        "\n",
        "* ***If there is more than one filter in the kernel.*** Note that for each filter, the W' matrix will be multiplied by the same image vector. This means that it is possible to concatenate the kernel filter matrices vertically and, in one multiplication, obtain the answer for all filters.\n",
        "![](https://ucarecdn.com/91757315-13b9-439c-a59d-9fa14629ce52/)\n",
        "\n",
        "* ***If there is more than one image at the input:*** note that the matrix W’ is the same for all batch images, that is you can first stretch each image into a column, and then concatenate these columns horizontally for all batch images.\n",
        "![](https://ucarecdn.com/e8a10b8d-876e-44cb-ad5c-2a56abafa974/)\n",
        "\n",
        "* ***If there is more than one layer in the image*** first we perform input and kernel transformations for each layer and then we concatenate: the vectors of different input layers into one large vector and the kernel matrices, respectively into one long matrix. And we will get the addition from the outputs by layers in the process of matrix multiplication.\n",
        "![](https://lh5.googleusercontent.com/2M0cSgkwRnEMQ8y2mrnD-D2alEYn3vVsX7UrgRNLV9BbYv6nswIWesOpKjjNpPMgUl0ixOZoUVyeZXHy5Jlfy1bS4lLkrLuo2ZmOH1gYh88aMKgKa_mjrZHAWzYbBtWihg8GDrxK)\n",
        "\n",
        "***That is even in the most general case, we can get the answer with one matrix multiplication.***\n",
        "\n",
        "But the output calculated in this way does not coincide in dimension with the output of the standard layer from PyTorch - you need to change the dimension.\n",
        "\n",
        "\n",
        "The code has already implemented:\n",
        "\n",
        "* transforming the input batch of images\n",
        "* multiplying the kernel matrix by the input matrix\n",
        "* response conversion\n",
        "\n",
        "Reminder: In all steps of this tutorial we consider the bias in the convolution layers to be zero.\n",
        "\n",
        "***All you have to do is convert the kernel to the format described above.***\n",
        "\n",
        "***Please note that the code considers the general case - the input consists of several multi-layer images there are several layers in the kernel.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1380c2c7",
      "metadata": {
        "id": "1380c2c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "319fe06f-30fe-42eb-eaa8-dd90206c21c8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d108a675fe03>\u001b[0m in \u001b[0;36m<cell line: 125>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m                                           output_height, output_width))\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_conv2d_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2dMatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-d108a675fe03>\u001b[0m in \u001b[0;36mtest_conv2d_layer\u001b[0;34m(conv2d_layer_class, batch_size, input_height, input_width, stride)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     custom_conv2d_out = create_and_call_conv2d_layer(\n\u001b[0m\u001b[1;32m     74\u001b[0m         conv2d_layer_class, stride, kernel, input_tensor)\n\u001b[1;32m     75\u001b[0m     conv2d_out = create_and_call_conv2d_layer(\n",
            "\u001b[0;32m<ipython-input-18-d108a675fe03>\u001b[0m in \u001b[0;36mcreate_and_call_conv2d_layer\u001b[0;34m(conv2d_layer_class, stride, kernel, input_matrix)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-d108a675fe03>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, torch_input)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mkernel_unsqueezed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unsqueeze_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_unsqueezed\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mtorch_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         return result.permute(1, 0).view((batch_size, self.out_channels,\n\u001b[1;32m    123\u001b[0m                                           output_height, output_width))\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x27 and 48x2)"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "\n",
        "def calc_out_shape(input_matrix_shape, out_channels, kernel_size, stride, padding):\n",
        "    batch_size, channels_count, input_height, input_width = input_matrix_shape\n",
        "    output_height = (input_height + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
        "    output_width = (input_width + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
        "\n",
        "    return batch_size, out_channels, output_height, output_width\n",
        "\n",
        "\n",
        "class ABCConv2d(ABC):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def set_kernel(self, kernel):\n",
        "        self.kernel = kernel\n",
        "\n",
        "    @abstractmethod\n",
        "    def __call__(self, input_tensor):\n",
        "        pass\n",
        "\n",
        "\n",
        "class Conv2d(ABCConv2d):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n",
        "                                      stride, padding=0, bias=False)\n",
        "\n",
        "    def set_kernel(self, kernel):\n",
        "        self.conv2d.weight.data = kernel\n",
        "\n",
        "    def __call__(self, input_tensor):\n",
        "        return self.conv2d(input_tensor)\n",
        "\n",
        "\n",
        "def create_and_call_conv2d_layer(conv2d_layer_class, stride, kernel, input_matrix):\n",
        "    out_channels = kernel.shape[0]\n",
        "    in_channels = kernel.shape[1]\n",
        "    kernel_size = kernel.shape[2]\n",
        "\n",
        "    layer = conv2d_layer_class(in_channels, out_channels, kernel_size, stride)\n",
        "    layer.set_kernel(kernel)\n",
        "\n",
        "    return layer(input_matrix)\n",
        "\n",
        "\n",
        "def test_conv2d_layer(conv2d_layer_class, batch_size=2,\n",
        "                      input_height=4, input_width=4, stride=2):\n",
        "    kernel = torch.tensor(\n",
        "                      [[[[0., 1, 0],\n",
        "                         [1,  2, 1],\n",
        "                         [0,  1, 0]],\n",
        "\n",
        "                        [[1, 2, 1],\n",
        "                         [0, 3, 3],\n",
        "                         [0, 1, 10]],\n",
        "\n",
        "                        [[10, 11, 12],\n",
        "                         [13, 14, 15],\n",
        "                         [16, 17, 18]]]])\n",
        "\n",
        "    in_channels = kernel.shape[1]\n",
        "\n",
        "    input_tensor = torch.arange(0, batch_size * in_channels *\n",
        "                                input_height * input_width,\n",
        "                                out=torch.FloatTensor()) \\\n",
        "        .reshape(batch_size, in_channels, input_height, input_width)\n",
        "\n",
        "    custom_conv2d_out = create_and_call_conv2d_layer(\n",
        "        conv2d_layer_class, stride, kernel, input_tensor)\n",
        "    conv2d_out = create_and_call_conv2d_layer(\n",
        "        Conv2d, stride, kernel, input_tensor)\n",
        "\n",
        "    return torch.allclose(custom_conv2d_out, conv2d_out) \\\n",
        "             and (custom_conv2d_out.shape == conv2d_out.shape)\n",
        "\n",
        "\n",
        "class Conv2dMatrix(ABCConv2d):\n",
        "    # Function to convert the kernel into a matrix of the desired type.\n",
        "    def _unsqueeze_kernel(self, torch_input, output_height, output_width):\n",
        "        kernel_unsqueezed = torch.zeros(output_height, output_width) # Implement a function that returns the converted kernel.\n",
        "\n",
        "        # не успела написать такой трансофрмирующий алгоритм, который бы работал при всех случаях и размерах\n",
        "        return kernel_unsqueezed\n",
        "\n",
        "    def __call__(self, torch_input):\n",
        "        batch_size, out_channels, output_height, output_width\\\n",
        "            = calc_out_shape(\n",
        "                input_matrix_shape=torch_input.shape,\n",
        "                out_channels=self.kernel.shape[0],\n",
        "                kernel_size=self.kernel.shape[2],\n",
        "                stride=self.stride,\n",
        "                padding=0)\n",
        "\n",
        "        kernel_unsqueezed = self._unsqueeze_kernel(torch_input, output_height, output_width)\n",
        "        result = kernel_unsqueezed @ torch_input.view((batch_size, -1)).permute(1, 0)\n",
        "        return result.permute(1, 0).view((batch_size, self.out_channels,\n",
        "                                          output_height, output_width))\n",
        "\n",
        "print(test_conv2d_layer(Conv2dMatrix))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fb7e867",
      "metadata": {
        "id": "9fb7e867"
      },
      "source": [
        "## Problem 5 (5 Point)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0bc4b6c",
      "metadata": {
        "id": "f0bc4b6c"
      },
      "source": [
        "In the previous problem, W' has many zeros. This reduces the effectiveness of the method.\n",
        "\n",
        "This task will involve implementation through matrices in a different, more efficient way.\n",
        "\n",
        "Let this time the input be a batch of one three-layer (RGB) image of size 3*3.\n",
        "\n",
        "Let the kernel have 2 filters with a width and a height of 2 pixels.\n",
        "\n",
        "Then the output should have dimension 1*2*2*2.\n",
        "\n",
        "Let W be the kernel weights X the values of the input matrix Y the output values.\n",
        "\n",
        "For simplicity the image layers and kernel filter layers are colored.\n",
        "\n",
        "***Please note*** for example the “blue” X0 does not have to be equal to the “red” X0 the same applies to the values in kernel filters - different colors and the same variables can have different values this designation was chosen so as not to clutter the figure with complex indices.\n",
        "![](https://ucarecdn.com/ddc6ccbe-2aef-4c7b-a47b-a15e67d3f6ec/)\n",
        "\n",
        "If in the first matrix method we pulled out images into columns, now we will pull out kernel filters into rows.\n",
        "![](https://ucarecdn.com/afbc3c3c-a347-4248-b0de-cd614e637fc3/)\n",
        "\n",
        "***We recommend checking on a piece of paper that the result of the product of such matrices gives the same result as the convolution.***\n",
        "\n",
        "Let's move from the simple case to the general one:\n",
        "\n",
        "* ***If there is more than one image in a batch*** the kernel transformation does not change, and the transformed input image matrices are concatenated horizontally.\n",
        "\n",
        "But the output calculated in this way does not coincide in dimension with the output of the standard layer from PyTorch - you need to change the dimension.\n",
        "\n",
        "The matrix multiplication function has already been implemented.\n",
        "\n",
        "Reminder: In all steps of this tutorial we consider the bias in the convolution layers to be zero.\n",
        "\n",
        "***You need to write kernel and input conversion functions.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "730385dc",
      "metadata": {
        "id": "730385dc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "\n",
        "def calc_out_shape(input_matrix_shape, out_channels, kernel_size, stride, padding):\n",
        "    batch_size, channels_count, input_height, input_width = input_matrix_shape\n",
        "    output_height = (input_height + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
        "    output_width = (input_width + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
        "\n",
        "    return batch_size, out_channels, output_height, output_width\n",
        "\n",
        "\n",
        "class ABCConv2d(ABC):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def set_kernel(self, kernel):\n",
        "        self.kernel = kernel\n",
        "\n",
        "    @abstractmethod\n",
        "    def __call__(self, input_tensor):\n",
        "        pass\n",
        "\n",
        "\n",
        "def create_and_call_conv2d_layer(conv2d_layer_class, stride, kernel, input_matrix):\n",
        "    out_channels = kernel.shape[0]\n",
        "    in_channels = kernel.shape[1]\n",
        "    kernel_size = kernel.shape[2]\n",
        "\n",
        "    layer = conv2d_layer_class(in_channels, out_channels, kernel_size, stride)\n",
        "    layer.set_kernel(kernel)\n",
        "\n",
        "    return layer(input_matrix)\n",
        "\n",
        "\n",
        "class Conv2d(ABCConv2d):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n",
        "                                      stride, padding=0, bias=False)\n",
        "\n",
        "    def set_kernel(self, kernel):\n",
        "        self.conv2d.weight.data = kernel\n",
        "\n",
        "    def __call__(self, input_tensor):\n",
        "        return self.conv2d(input_tensor)\n",
        "\n",
        "\n",
        "def test_conv2d_layer(conv2d_layer_class, batch_size=2,\n",
        "                      input_height=4, input_width=4, stride=2):\n",
        "    kernel = torch.tensor(\n",
        "                      [[[[0., 1, 0],\n",
        "                         [1,  2, 1],\n",
        "                         [0,  1, 0]],\n",
        "\n",
        "                        [[1, 2, 1],\n",
        "                         [0, 3, 3],\n",
        "                         [0, 1, 10]],\n",
        "\n",
        "                        [[10, 11, 12],\n",
        "                         [13, 14, 15],\n",
        "                         [16, 17, 18]]]])\n",
        "\n",
        "    in_channels = kernel.shape[1]\n",
        "\n",
        "    input_tensor = torch.arange(0, batch_size * in_channels *\n",
        "                                input_height * input_width,\n",
        "                                out=torch.FloatTensor()) \\\n",
        "        .reshape(batch_size, in_channels, input_height, input_width)\n",
        "\n",
        "    custom_conv2d_out = create_and_call_conv2d_layer(\n",
        "        conv2d_layer_class, stride, kernel, input_tensor)\n",
        "    conv2d_out = create_and_call_conv2d_layer(\n",
        "        Conv2d, stride, kernel, input_tensor)\n",
        "\n",
        "    return torch.allclose(custom_conv2d_out, conv2d_out) \\\n",
        "             and (custom_conv2d_out.shape == conv2d_out.shape)\n",
        "\n",
        "\n",
        "class Conv2dMatrixV2(ABCConv2d):\n",
        "    # Function for converting the kernel to the required format.\n",
        "    def _convert_kernel(self):\n",
        "        converted_kernel = # Implement kernel conversion.\n",
        "        # не успела\n",
        "        return converted_kernel\n",
        "\n",
        "    # Function for converting input to the desired format.\n",
        "    def _convert_input(self, torch_input, output_height, output_width):\n",
        "        converted_input = # Implement input transformation.\n",
        "        # не успела\n",
        "        return converted_input\n",
        "\n",
        "    def __call__(self, torch_input):\n",
        "        batch_size, out_channels, output_height, output_width\\\n",
        "            = calc_out_shape(\n",
        "                input_matrix_shape=torch_input.shape,\n",
        "                out_channels=self.kernel.shape[0],\n",
        "                kernel_size=self.kernel.shape[2],\n",
        "                stride=self.stride,\n",
        "                padding=0)\n",
        "\n",
        "        converted_kernel = self._convert_kernel()\n",
        "        converted_input = self._convert_input(torch_input, output_height, output_width)\n",
        "\n",
        "        conv2d_out_alternative_matrix_v2 = converted_kernel @ converted_input\n",
        "        return conv2d_out_alternative_matrix_v2.transpose(1,0).view(torch_input.shape[0],\n",
        "                                                     self.out_channels,\n",
        "                                                     output_height,\n",
        "                                                     output_width)\n",
        "\n",
        "print(test_conv2d_layer(Conv2dMatrixV2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d6fe93d",
      "metadata": {
        "id": "9d6fe93d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}